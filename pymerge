#!/usr/bin/env python3
"""A program for merging GRAW files

This program can be used to merge a set of GRAW files into one Event file, and to apply some pre-processing to the
raw data.

For usage information, run

    pymerge -h

"""

import multiprocessing as mp
import logging
import numpy as np
from clint.textui import puts, indent, progress
import os
import glob
import sys
import argparse
import scipy.stats
from functools import reduce
import copy
from collections import deque
import pytpc


def subtract_and_delete_fpn(traces):
    """Subtract the normalized, average fixed-pattern noise from the data.

    The FPN channels for each AGET are averaged, renormalized to zero, and subtracted
    from the data signals in that AGET. They are then deleted from the data.

    Parameters
    ----------
    traces : np.ndarray
        The structured NumPy array from the event, e.g. `evt.traces`

    Returns
    -------
    ndarray
        The same structured array as before, but with the FPN subtracted and deleted.

    """

    traces = copy.deepcopy(traces)

    fpn_channels = [11, 22, 45, 56]

    for cobo, asad, aget in {tuple(a) for a in traces[['cobo', 'asad', 'aget']]}:
        fpn_idx = np.where(np.all((traces['cobo'] == cobo, traces['asad'] == asad,
                                   traces['aget'] == aget, np.in1d(traces['channel'], fpn_channels)), axis=0))[0]
        data_idx = np.where(np.all((traces['cobo'] == cobo, traces['asad'] == asad,
                                    traces['aget'] == aget, ~np.in1d(traces['channel'], fpn_channels)), axis=0))[0]

        if len(fpn_idx) != 4:
            logger.warn('Number of FPN channels was incorrect: %d (should be 4)' % len(fpn_idx))

        mean_fpn = traces['data'][fpn_idx].mean(axis=0)
        mean_fpn -= mean_fpn.mean()

        traces['data'][data_idx] -= mean_fpn

    return np.delete(traces, np.where(np.in1d(traces['channel'], fpn_channels)))


class EventProcessor(mp.Process):
    """Worker process for processing the raw data into events.

    This is where the bulk of the work is done in the code.

    To stop the process, put the string 'STOP' onto `inq`.

    Parameters
    ----------
    lookup : dict-like
        The pad mapping lookup table
    peds : array-like
        The pedestals, as a list indexed by pad number
    threshold : number or None
        The threshold to apply. No thresholding is done if this is None.
    inq : multiprocessing.JoinableQueue
        The queue of inputs, which are tuples of (evtid, timestamp, frames)
    outq : multiprocessing.JoinableQueue
        The output queue to put the processed, merged events onto
    """
    def __init__(self, lookup, peds, threshold, inq, outq):
        self.lookup = lookup
        self.peds = peds
        self.threshold = threshold
        self.inq = inq
        self.outq = outq
        super().__init__()

    def process_event(self, evtid, timestamp, rawframes):
        """Process the raw frames into an event.

        This takes raw frames (really just a chunk of binary data) and unpacks and merges them into an event. Then,
        optionally, the FPN and pedestals are subtracted and a threshold is applied.

        Parameters
        ----------
        evtid : number
            The event ID
        timestamp : number
            The time stamp of the event
        rawframes : iterable
            The raw binary data of the frames

        Returns
        -------
        evt : pytpc.evtdata.Event
            The processed event
        """
        frames = [pytpc.grawdata.GRAWFile._parse(r) for r in rawframes]
        evt = pytpc.Event(evtid, timestamp)
        evt.traces = np.concatenate(frames)

        evt.traces['pad'] = [self.lookup.get(tuple(a), 20000) for a in evt.traces[['cobo', 'asad', 'aget', 'channel']]]

        evt.traces = subtract_and_delete_fpn(evt.traces)

        bad_channels = np.where(evt.traces['pad'] == 20000)[0]
        if len(bad_channels) != 0:
            addr_list = '[' + ', '.join(('{}/{}/{}/{}'.format(evt.traces['cobo'][i], evt.traces['asad'][i],
                                                              evt.traces['aget'][i], evt.traces['channel'][i])
                                                              for i in bad_channels)) + ']'
            logger.warn('Event %d contained unmapped channels: %s', evtid, addr_list)
            evt.traces = np.delete(evt.traces, bad_channels)

        if self.peds is not None:
            evt.traces['data'] = (evt.traces['data'].T - self.peds[evt.traces['pad']]).T

        if self.threshold is not None:
            evt.traces['data'] = scipy.stats.threshold(evt.traces['data'], threshmin=self.threshold)

        return evt

    def run(self):
        """Main function for this subprocess.
        """
        while True:
            try:
                # logger.debug('Getting input from inq')
                inp = self.inq.get()
                # logger.debug('Got input from inq')
                if isinstance(inp, str) and inp == 'STOP':
                    logger.debug('Worker received STOP')
                    return
                else:
                    evtid, timestamp, frames = inp
            except TypeError:
                logger.exception('TypeError in worker when receiving frames')
            except (KeyboardInterrupt, SystemExit):
                logger.debug('Got interrupted. Dying.')
                return
            finally:
                self.inq.task_done()

            try:
                evt = self.process_event(evtid, timestamp, frames)
                logger.debug('Processed event %d', evtid)
            except (KeyboardInterrupt, SystemExit):
                logger.debug('Got interrupted. Dying.')
                return
            except:
                logger.exception('Exception in event processing')
            else:
                logger.debug('Waiting to put event on outq')
                self.outq.put(evt)
                logger.debug('Finished putting event on outq')


class WriterProcess(mp.Process):
    """The subprocess for writing the events to the event file.

    This pulls events off of the `outq` and writes them to the given file.

    To stop the process, put the string 'STOP' onto `outq`.

    Parameters
    ----------
    outq : multiprocessing.JoinableQueue
        The output queue, which contains the events to be written to disk
    filename : string
        The path to the file to write
    """
    def __init__(self, outq, filename):
        self.filename = filename
        self.outq = outq
        super().__init__()

    def run(self):
        """The main function for the writer process.
        """
        outfile = pytpc.evtdata.EventFile(self.filename, 'w')
        num_events_processed = 0
        while True:
            try:
                # logger.debug('Getting output')
                r = self.outq.get()
                # logger.debug('Got output')
                if isinstance(r, str) and r == 'STOP':
                    logger.debug('Writer received stop')
                    return
                else:
                    outfile.write(r)
                    logger.debug('Wrote event %d', r.evt_id)
            except (KeyboardInterrupt, SystemExit):
                logger.debug('Got interrupted. Dying.')
                return
            finally:
                self.outq.task_done()
                num_events_processed += 1


def index_file(path):
    """Open a GRAW file for the first time, causing it to be indexed.

    If the file is already indexed, this should return quickly. This allows us to know how many frames are
    in the file.

    Parameters
    ----------
    path : string
        The path to the GRAW file

    Returns
    -------
    file_name : string
        The name of the file
    integer
        The number of frames in the file
    """
    gf = pytpc.grawdata.GRAWFile(path)
    file_name = os.path.basename(path)
    return file_name, len(gf)


def main():
    """The main function for the controlling process.
    """
    # Parse the command line options

    parser = argparse.ArgumentParser(description='A program for merging GRAW files.')
    parser.add_argument('input', type=str,
                        help='Path to a directory of GRAW files')
    parser.add_argument('output', type=str, nargs='?',
                        help='Path to the output file')
    parser.add_argument('--lookup', '-l', type=str,
                        help='Path to the pad lookup table, as a CSV file')
    parser.add_argument('--pedestals', '-p', type=str,
                        help='Path to a table of pedestal values')
    parser.add_argument('--threshold', '-t', type=int,
                        help='Threshold value')
    parser.add_argument('--verbose', '-v', action='count', default=0,
                        help='Print more information')
    args = parser.parse_args()

    if args.verbose == 1:
        logger.setLevel(logging.INFO)
    elif args.verbose >= 2:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.WARN)

    # Determine the number of cores available

    try:
        num_cores = mp.cpu_count()  # according to the docs, this might raise NotImplementedError
        logger.debug('CPU count is %d', num_cores)
    except NotImplementedError:
        num_cores = 4  # this seems like a good assumption for a modern system
        logger.warn("Couldn't get cpu count. Defaulting to %d", num_cores)

    # Find and map the GRAW files

    print('Looking for files')
    gfile_paths = glob.glob(os.path.join(os.path.abspath(args.input), '*.graw'))
    with indent(4):
        for x in gfile_paths:
            puts('Found file: %s' % os.path.basename(x))
    print('Found {} GRAW files'.format(len(gfile_paths)))

    print('Mapping frames in files')

    num_frames = {}
    # with mp.Pool(processes=num_cores-1) as pool:
    #     index_res = pool.imap_unordered(index_file, gfile_paths)
    #     for n, i in progress.bar(index_res, expected_size=len(gfile_paths)):
    #         num_frames[n] = i
    for p in progress.bar(gfile_paths):
        n, i = index_file(p)
        num_frames[n] = i

    puts('Finished indexing. File length summary:')
    with indent(4):
        for n in sorted(num_frames.keys()):
            puts('{} contained {} frames'.format(n, num_frames[n]))

    # Load the lookup table and pedestals, and parse the threshold option

    if args.lookup is not None:
        print('Loading lookup table')
        lookup = {}
        with open(args.lookup) as lfile:
            for line in lfile:
                cobo, asad, aget, ch, pad = [int(a) for a in line.strip().split(',')]
                if -1 in (cobo, asad, aget, ch, pad):
                    continue
                lookup[(cobo, asad, aget, ch)] = pad
    else:
        lookup = {}

    if args.pedestals is not None:
        print('Loading pedestals')
        peds = np.zeros(10240)
        with open(args.pedestals) as pfile:
            for line in pfile:
                pad, ped = line.strip().split(',')
                peds[int(pad)] = float(ped) if '.' in ped else int(ped)
    else:
        peds = None

    if args.threshold is not None:
        threshold = args.threshold
    else:
        threshold = None

    # Now open the files for reading

    gfiles = [pytpc.grawdata.GRAWFile(g) for g in gfile_paths]
    valid_events = sorted(reduce(set.union, [set(g.evtids) for g in gfiles]))  # TODO: Make this check for frames that are in all cobos

    logger.debug('Length of set of valid events was %d', len(valid_events))

    if args.output is not None:
        output_name = args.output
    else:
        run_name = os.path.split(os.path.normpath(args.input))[-1]
        output_name = run_name + '.evt'

    # Prepare the queues and worker/writer processes

    logger.debug('Making queues')

    inq = mp.JoinableQueue(50)
    outq = mp.JoinableQueue(50)

    logger.debug('Making processes')

    num_workers = max(num_cores - 1, 1)  # need to have at least one worker
    logger.debug('Starting %d worker processes', num_workers)

    workers = [EventProcessor(lookup, peds, threshold, inq, outq) for i in range(num_workers)]
    for w in workers:
        w.start()
    writer = WriterProcess(outq, output_name)
    writer.start()

    # Now start putting frames on the input queue

    print('Beginning merge')

    for i in progress.bar(valid_events):
        frames = []
        for g in gfiles:
            this_frames = g.get_raw_frames_for_event(i)
            frames += this_frames

        if len(frames) == 0:
            logger.warn('No frames in event %d', i)
            continue
        else:
            logging.debug('Found %d frames for event %d', len(frames), i)

        inq.put((i, 0, frames))
        logger.debug('Put in frames for event %d', i)

    # Wrap up the queues and worker/writer processes

    logger.debug('Waiting for all jobs to finish')
    inq.join()  # blocks main thread until queue is empty and all tasks on it are finished
    outq.join()

    for i in range(num_workers):
        inq.put('STOP')  # This tells the processes to die
    outq.put('STOP')

    inq.join()  # block until all of the STOPs have been taken out
    outq.join()

    for proc in workers:
        proc.join()
    writer.join()


if __name__ == '__main__':
    logger = mp.log_to_stderr()
    main()
