#!/usr/bin/env python3
"""A program for merging GRAW files

This program can be used to merge a set of GRAW files into one Event file, and to apply some pre-processing to the
raw data.

For usage information, run

    pymerge -h

"""

import multiprocessing as mp
import logging
import numpy as np
from clint.textui import puts, indent, progress
import os
import glob
import sys
import argparse
import scipy.stats
from functools import reduce
import copy
import pytpc
import re


def subtract_and_delete_fpn(traces):
    """Subtract the normalized, average fixed-pattern noise from the data.

    The FPN channels for each AGET are averaged, renormalized to zero, and subtracted
    from the data signals in that AGET. They are then deleted from the data.

    Parameters
    ----------
    traces : np.ndarray
        The structured NumPy array from the event, e.g. `evt.traces`

    Returns
    -------
    ndarray
        The same structured array as before, but with the FPN subtracted and deleted.

    """

    traces = copy.deepcopy(traces)

    fpn_channels = [11, 22, 45, 56]

    for cobo, asad, aget in {tuple(a) for a in traces[['cobo', 'asad', 'aget']]}:
        fpn_idx = np.where(np.all((traces['cobo'] == cobo, traces['asad'] == asad,
                                   traces['aget'] == aget, np.in1d(traces['channel'], fpn_channels)), axis=0))[0]
        data_idx = np.where(np.all((traces['cobo'] == cobo, traces['asad'] == asad,
                                    traces['aget'] == aget, ~np.in1d(traces['channel'], fpn_channels)), axis=0))[0]

        if len(fpn_idx) != 4:
            logger.warn('Number of FPN channels was incorrect: %d (should be 4)' % len(fpn_idx))

        mean_fpn = traces['data'][fpn_idx].mean(axis=0)
        mean_fpn -= mean_fpn.mean()

        traces['data'][data_idx] -= mean_fpn

    return np.delete(traces, np.where(np.in1d(traces['channel'], fpn_channels)))


class EventProcessor(mp.Process):
    """Worker process for processing the raw data into events.

    This is where the bulk of the work is done in the code.

    To stop the process, put the string 'STOP' onto `inq`.

    Parameters
    ----------
    lookup : dict-like
        The pad mapping lookup table
    peds : array-like
        The pedestals, as a list indexed by pad number
    threshold : number or None
        The threshold to apply. No thresholding is done if this is None.
    inq : multiprocessing.JoinableQueue
        The queue of inputs, which are tuples of (evtid, timestamp, frames)
    outq : multiprocessing.JoinableQueue
        The output queue to put the processed, merged events onto
    """
    def __init__(self, lookup, peds, threshold, inq, outq):
        self.lookup = lookup
        self.peds = peds
        self.threshold = threshold
        self.inq = inq
        self.outq = outq
        super().__init__()

    def process_event(self, evtid, timestamp, rawframes):
        """Process the raw frames into an event.

        This takes raw frames (really just a chunk of binary data) and unpacks and merges them into an event. Then,
        optionally, the FPN and pedestals are subtracted and a threshold is applied.

        Parameters
        ----------
        evtid : number
            The event ID
        timestamp : number
            The time stamp of the event
        rawframes : iterable
            The raw binary data of the frames

        Returns
        -------
        evt : pytpc.evtdata.Event
            The processed event
        """
        frames = [pytpc.grawdata.GRAWFile._parse(r) for r in rawframes]
        evt = pytpc.Event(evtid, timestamp)
        evt.traces = np.concatenate(frames)

        evt.traces['pad'] = [self.lookup.get(tuple(a), 20000) for a in evt.traces[['cobo', 'asad', 'aget', 'channel']]]

        evt.traces = subtract_and_delete_fpn(evt.traces)

        bad_channels = np.where(evt.traces['pad'] == 20000)[0]
        if len(bad_channels) != 0:
            addr_list = '[' + ', '.join(('{}/{}/{}/{}'.format(evt.traces['cobo'][i], evt.traces['asad'][i],
                                                              evt.traces['aget'][i], evt.traces['channel'][i])
                                         for i in bad_channels)) + ']'
            logger.warn('Event %d contained unmapped channels: %s', evtid, addr_list)
            evt.traces = np.delete(evt.traces, bad_channels)

        if self.peds is not None:
            evt.traces['data'] = (evt.traces['data'].T - self.peds[evt.traces['pad']]).T

        if self.threshold is not None:
            evt.traces['data'] = scipy.stats.threshold(evt.traces['data'], threshmin=self.threshold)

        return evt

    def run(self):
        """Main function for this subprocess.
        """
        while True:
            try:
                inp = self.inq.get()
                if isinstance(inp, str) and inp == 'STOP':
                    logger.debug('Worker received STOP')
                    return
                else:
                    try:
                        evtid, timestamp, frames = inp
                    except TypeError:
                        logger.exception('TypeError in worker when receiving frames')
                    else:
                        evt = self.process_event(evtid, timestamp, frames)
                        logger.debug('Processed event %d', evtid)
                        logger.debug('Waiting to put event on outq')
                        self.outq.put(evt)
                        logger.debug('Finished putting event on outq')

            except (KeyboardInterrupt, SystemExit):
                logger.debug('Got interrupted. Dying.')
                return
            except:
                logger.exception('Exception in event processing')
            finally:
                self.inq.task_done()


class WriterProcess(mp.Process):
    """The subprocess for writing the events to the event file.

    This pulls events off of the `outq` and writes them to the given file.

    To stop the process, put the string 'STOP' onto `outq`.

    Parameters
    ----------
    outq : multiprocessing.JoinableQueue
        The output queue, which contains the events to be written to disk
    filename : string
        The path to the file to write
    """
    def __init__(self, outq, filename):
        self.filename = filename
        self.outq = outq
        super().__init__()

    def run(self):
        """The main function for the writer process.
        """
        outfile = pytpc.evtdata.EventFile(self.filename, 'w')
        num_events_processed = 0
        while True:
            try:
                # logger.debug('Getting output')
                r = self.outq.get()
                # logger.debug('Got output')
                if isinstance(r, str) and r == 'STOP':
                    logger.debug('Writer received stop')
                    return
                else:
                    outfile.write(r)
                    logger.debug('Wrote event %d', r.evt_id)
            except (KeyboardInterrupt, SystemExit):
                logger.debug('Got interrupted. Dying.')
                return
            finally:
                self.outq.task_done()
                num_events_processed += 1


def merge_partial_events(infiles, outfile_path):
    # Find the event IDs in each file
    all_evtids = np.sort(reduce(np.union1d, (f.evtids for f in infiles)))

    # Open the output file for writing
    outfile = pytpc.evtdata.EventFile(outfile_path, 'w')

    for evtid in progress.bar(all_evtids):
        partials = []
        for f in infiles:
            try:
                partials.append(f.get_by_event_id(evtid))
            except KeyError:
                continue
            except ValueError as e:
                raise ValueError('File %s had non-unique event numbers' % f.fp.name) from e

        full_evt = pytpc.evtdata.Event(evtid, partials[0].timestamp)
        full_evt.traces = np.concatenate([p.traces for p in partials])

        outfile.write(full_evt)

    outfile.close()


def index_file(path, is_raw):
    """Open a GRAW file for the first time, causing it to be indexed.

    If the file is already indexed, this should return quickly. This allows us to know how many frames are
    in the file.

    Parameters
    ----------
    path : string
        The path to the GRAW file
    is_raw : bool
        Is the file a GRAW file?

    Returns
    -------
    file_name : string
        The name of the file
    integer
        The number of frames in the file
    """
    if is_raw:
        f = pytpc.grawdata.GRAWFile(path)
    else:
        f = pytpc.evtdata.EventFile(path)
    file_name = os.path.basename(path)
    return file_name, len(f)


def find_files(input_arg):
    logger.debug('Finding files in %s', str(input_arg))
    raw_files = []
    evt_files = []
    for ia in input_arg:
        if os.path.isdir(ia):
            logger.info('Looking for files in directory %s', ia)
            graw_paths = glob.glob(os.path.join(os.path.abspath(ia), '*.graw'))
            dat_paths = glob.glob(os.path.join(os.path.abspath(ia), '*.dat*'))
            evt_paths = glob.glob(os.path.join(os.path.abspath(ia), '*.evt'))
            raw_files += graw_paths + dat_paths
            logger.info('Found %d raw files', len(graw_paths) + len(dat_paths))
            evt_files += evt_paths
            logger.info('Found %d evt files', len(evt_paths))
        elif os.path.isfile(ia):
            if re.match(r'.*\.graw|.*\.dat.*', ia):
                raw_files.append(os.path.abspath(ia))
                logger.info('Added specified raw file %s', ia)
            elif re.match(r'.*\.evt', ia):
                evt_files.append(os.path.abspath(ia))
                logger.info('Added specified evt file %s', ia)
            else:
                logger.warn('Specified file %s was of an unknown type', ia)
        elif os.path.islink(ia):
            if re.match(r'.*\.graw|.*\.dat.*', ia):
                raw_files.append(os.path.realpath(ia))
                logger.info('Resolved symlink %s to raw file', ia)
            elif re.match(r'.*\.evt', ia):
                evt_files.append(os.path.realpath(ia))
                logger.info('Resolved symlink %s to evt file', ia)
            else:
                logger.warn('Symlink %s points to file of unknown type', ia)
        else:
            logger.error('Input argument was not dir, path, or link: %s', ia)

    return raw_files, evt_files


def main():
    """The main function for the controlling process.
    """
    # Parse the command line options

    parser = argparse.ArgumentParser(description='A program for merging GRAW files.')
    parser.add_argument('input', type=str, nargs='+',
                        help='Path to a directory of GRAW files')
    parser.add_argument('--output', '-o', type=str,
                        help='Path to the output file')
    parser.add_argument('--lookup', '-l', type=str,
                        help='Path to the pad lookup table, as a CSV file')
    parser.add_argument('--pedestals', '-p', type=str,
                        help='Path to a table of pedestal values')
    parser.add_argument('--threshold', '-t', type=int,
                        help='Threshold value')
    parser.add_argument('--verbose', '-v', action='count', default=0,
                        help='Print more information')
    args = parser.parse_args()

    if args.verbose == 1:
        logger.setLevel(logging.INFO)
    elif args.verbose >= 2:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.WARN)

    # Determine the number of cores available

    try:
        num_cores = mp.cpu_count()  # according to the docs, this might raise NotImplementedError
        logger.debug('CPU count is %d', num_cores)
    except NotImplementedError:
        num_cores = 4  # this seems like a good assumption for a modern system
        logger.warn("Couldn't get cpu count. Defaulting to %d", num_cores)

    # Find and map the GRAW files

    print('Looking for files')
    gfile_paths, efile_paths = find_files(args.input)
    with indent(4):
        for x in (gfile_paths + efile_paths):
            puts('Found file: %s' % os.path.basename(x))

    if len(gfile_paths) > 0 and len(efile_paths) == 0:
        files_are_raw = True
        file_paths = gfile_paths
        print('Found {} GRAW files'.format(len(file_paths)))
    elif len(efile_paths) > 0 and len(gfile_paths) == 0:
        files_are_raw = False
        file_paths = efile_paths
        print('Found {} event files'.format(len(file_paths)))
    else:
        logger.critical('Inputs had both raw and processed files. Give one or the other, but not both.')
        sys.exit(1)

    print('Mapping frames in files')

    num_frames = {}
    for p in progress.bar(file_paths):
        n, i = index_file(p, files_are_raw)
        num_frames[n] = i

    puts('Finished indexing. File length summary:')
    with indent(4):
        for n in sorted(num_frames.keys()):
            puts('{} contained {} frames'.format(n, num_frames[n]))

    if files_are_raw:
        # Load the lookup table and pedestals, and parse the threshold option

        if args.lookup is not None:
            print('Loading lookup table')
            lookup = {}
            with open(args.lookup) as lfile:
                for line in lfile:
                    cobo, asad, aget, ch, pad = [int(a) for a in line.strip().split(',')]
                    if -1 in (cobo, asad, aget, ch, pad):
                        continue
                    lookup[(cobo, asad, aget, ch)] = pad
        else:
            lookup = {}

        if args.pedestals is not None:
            print('Loading pedestals')
            peds = np.zeros(10240)
            with open(args.pedestals) as pfile:
                for line in pfile:
                    pad, ped = line.strip().split(',')
                    peds[int(pad)] = float(ped) if '.' in ped else int(ped)
        else:
            peds = None

        if args.threshold is not None:
            threshold = args.threshold
        else:
            threshold = None

    # Now open the files for reading
    if files_are_raw:
        files = [pytpc.grawdata.GRAWFile(f) for f in file_paths]
    else:
        files = [pytpc.evtdata.EventFile(f) for f in file_paths]

    if args.output is not None:
        output_name = args.output
    else:
        if len(args.input) == 1 and os.path.isdir(args.input[0]):
            run_name = os.path.split(os.path.normpath(args.input))[-1]
            output_name = run_name + '.evt'
        else:
            output_name = 'out.evt'

    if not files_are_raw:
        merge_partial_events(files, output_name)
        return

    else:

        # Prepare the queues and worker/writer processes

        logger.debug('Making queues')

        inq = mp.JoinableQueue(50)
        outq = mp.JoinableQueue(50)

        logger.debug('Making processes')

        num_workers = max(num_cores - 1, 1)  # need to have at least one worker
        logger.debug('Starting %d worker processes', num_workers)

        workers = [EventProcessor(lookup, peds, threshold, inq, outq) for n in range(num_workers)]
        for w in workers:
            w.start()
        writer = WriterProcess(outq, output_name)
        writer.start()

        # Now start putting frames on the input queue

        print('Beginning merge')

        # TODO: Make this check for frames that are in all cobos
        valid_events = sorted(reduce(set.union, [set(f.evtids) for f in files]))
        logger.debug('Length of set of valid events was %d', len(valid_events))

        for i in progress.bar(valid_events):
            frames = []
            for g in files:
                this_frames = g.get_raw_frames_for_event(i)
                frames += this_frames

            if len(frames) == 0:
                logger.warn('No frames in event %d', i)
                continue
            else:
                logging.debug('Found %d frames for event %d', len(frames), i)

            inq.put((i, 0, frames))
            logger.debug('Put in frames for event %d', i)

        # Wrap up the queues and worker/writer processes

        logger.debug('Waiting for all jobs to finish')
        inq.join()  # blocks main thread until queue is empty and all tasks on it are finished
        outq.join()

        for i in range(num_workers):
            inq.put('STOP')  # This tells the processes to die
        outq.put('STOP')

        inq.join()  # block until all of the STOPs have been taken out
        outq.join()

        for proc in workers:
            proc.join()
        writer.join()


if __name__ == '__main__':
    logger = mp.log_to_stderr()
    main()
